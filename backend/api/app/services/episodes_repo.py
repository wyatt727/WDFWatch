"""Episode filesystem services for the unified Claude pipeline.

This module centralizes all episode file management logic so both the FastAPI
service layer and any remaining Python helpers share the same implementation.
The design keeps compatibility with previous helpers while removing legacy
fallbacks, always targeting the Claude pipeline directory structure.
"""

from __future__ import annotations

import hashlib
import json
import logging
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Union

from app.config import settings

logger = logging.getLogger(__name__)


# Mapping of logical file keys to on-disk filenames used by the Claude pipeline.
CLAUDE_FILE_MAP: Dict[str, str] = {
    "transcript": "transcript.txt",
    "video_url": "video_url.txt",
    "overview": "podcast_overview.txt",
    "episode_context": "EPISODE_CONTEXT.md",
    "summary": "summary.md",
    "keywords": "keywords.json",
    "tweets": "tweets.json",
    "classified": "classified.json",
    "responses": "responses.json",
    "published": "published.json",
}

# Inputs that come from the user or upstream systems. Everything else is an
# output generated by the pipeline.
INPUT_KEYS: Iterable[str] = {"transcript", "video_url", "overview"}

# Default collection of outputs we allow the user to clean up.
DEFAULT_OUTPUT_KEYS: Iterable[str] = (
    "summary",
    "keywords",
    "episode_context",
    "tweets",
    "classified",
    "responses",
    "published",
)


class EpisodesRepository:
    """Lightweight repository for episode directory operations."""

    def __init__(self) -> None:
        self.episodes_dir = settings.EPISODES_DIR
        self.episodes_dir.mkdir(parents=True, exist_ok=True)

    def get_episode_dir(self, episode_id: str) -> Path:
        """Return the path for a given episode identifier."""
        return self.episodes_dir / episode_id

    def ensure_episode_dir(self, episode_id: str) -> Path:
        """Ensure the directory exists before returning it."""
        episode_dir = self.get_episode_dir(episode_id)
        episode_dir.mkdir(parents=True, exist_ok=True)
        return episode_dir

    def episode_exists(self, episode_id: str) -> bool:
        """Check if an episode directory is present on disk."""
        return self.get_episode_dir(episode_id).exists()

    def get_episode_files(self, episode_id: str) -> List[Dict[str, Any]]:
        """List files within an episode directory for API serialization."""
        episode_dir = self.get_episode_dir(episode_id)
        if not episode_dir.exists():
            return []

        files: List[Dict[str, Any]] = []
        for file_path in sorted(episode_dir.iterdir()):
            if not file_path.is_file():
                continue

            stat = file_path.stat()
            files.append(
                {
                    "filename": file_path.name,
                    "path": str(file_path),
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                }
            )

        return files

    def read_episode_file(self, episode_id: str, filename: str) -> Optional[str]:
        """Read a text file if it exists; otherwise return ``None``."""
        file_path = self.get_episode_dir(episode_id) / filename
        if not file_path.exists():
            return None
        return file_path.read_text(encoding="utf-8")

    def write_episode_file(self, episode_id: str, filename: str, content: str) -> Path:
        """Write a UTF-8 file to disk and return its final path."""
        episode_dir = self.ensure_episode_dir(episode_id)
        file_path = episode_dir / filename
        file_path.write_text(content, encoding="utf-8")
        return file_path

    def read_json_file(self, episode_id: str, filename: str) -> Optional[Dict[str, Any]]:
        """Read a JSON file from disk and deserialize it."""
        raw = self.read_episode_file(episode_id, filename)
        if raw is None:
            return None
        return json.loads(raw)

    def write_json_file(self, episode_id: str, filename: str, data: Dict[str, Any]) -> Path:
        """Serialize JSON data to the episode directory."""
        return self.write_episode_file(episode_id, filename, json.dumps(data, indent=2))

    def list_episodes(self) -> List[str]:
        """Return a sorted list of visible episode IDs."""
        if not self.episodes_dir.exists():
            return []

        return sorted(
            item.name
            for item in self.episodes_dir.iterdir()
            if item.is_dir() and not item.name.startswith(".")
        )


@dataclass
class FileConfig:
    """Simple representation of configured episode files."""

    episode_dir: str
    files: Dict[str, str] = field(default_factory=lambda: dict(CLAUDE_FILE_MAP))

    def ensure_defaults(self) -> None:
        """Guarantee every known key exists in the mapping."""
        for key, value in CLAUDE_FILE_MAP.items():
            self.files.setdefault(key, value)


class EpisodeFileManager:
    """Manage episode-scoped files using Claude pipeline conventions."""

    CONFIG_FILENAME = "pipeline-config.json"

    def __init__(
        self,
        episode_id: Union[int, str],
        *,
        episode_dir: Optional[str] = None,
        repo: EpisodesRepository = None,
    ) -> None:
        self.repo = repo or episodes_repo
        self.episode_id = str(episode_id)
        # Default directory name mirrors the supplied identifier; callers can
        # still pass an explicit directory when needed (e.g., persisted value).
        self.episode_dir = episode_dir or self.episode_id

        self.base_path = self.repo.ensure_episode_dir(self.episode_dir)
        self.file_config = self._load_or_create_config()

        logger.info(
            "Episode file manager initialized",
            episode_id=self.episode_id,
            episode_dir=self.episode_dir,
            base_path=str(self.base_path),
        )

    # ------------------------------------------------------------------
    # Configuration helpers
    # ------------------------------------------------------------------
    def _config_path(self) -> Path:
        return self.base_path / self.CONFIG_FILENAME

    def _load_or_create_config(self) -> FileConfig:
        config_path = self._config_path()
        if config_path.exists():
            try:
                data = json.loads(config_path.read_text(encoding="utf-8"))
                file_config = FileConfig(**data)
                file_config.ensure_defaults()
                return file_config
            except Exception as error:  # pragma: no cover - defensive logging
                logger.warning(
                    "Failed to load pipeline-config.json; regenerating",
                    episode_id=self.episode_id,
                    error=str(error),
                )

        file_config = FileConfig(episode_dir=self.episode_dir)
        file_config.ensure_defaults()
        self._save_config(file_config)
        return file_config

    def _save_config(self, config: FileConfig) -> None:
        config_path = self._config_path()
        config_path.parent.mkdir(parents=True, exist_ok=True)
        config_path.write_text(json.dumps({
            "episode_dir": config.episode_dir,
            "files": config.files,
        }, indent=2), encoding="utf-8")

    # ------------------------------------------------------------------
    # Path helpers
    # ------------------------------------------------------------------
    def get_input_path(self, key: str) -> Path:
        return self._get_path_for_key(key, expect_input=True)

    def get_output_path(self, key: str) -> Path:
        return self._get_path_for_key(key, expect_input=False)

    def _get_path_for_key(self, key: str, *, expect_input: bool) -> Path:
        relative_path = self.file_config.files.get(key)
        if not relative_path:
            raise ValueError(f"Unknown file key: {key}")

        path = self.base_path / relative_path
        if expect_input:
            path.parent.mkdir(parents=True, exist_ok=True)
        else:
            path.parent.mkdir(parents=True, exist_ok=True)
        return path

    # ------------------------------------------------------------------
    # File operations
    # ------------------------------------------------------------------
    def read_input(self, key: str, *, encoding: str = "utf-8") -> str:
        path = self.get_input_path(key)
        if not path.exists():
            raise FileNotFoundError(f"Input file not found: {key} ({path})")
        return path.read_text(encoding=encoding)

    def write_output(
        self,
        key: str,
        content: Union[str, Dict[str, Any], List[Any]],
        *,
        encoding: str = "utf-8",
    ) -> None:
        path = self.get_output_path(key)
        if isinstance(content, (dict, list)):
            path.write_text(json.dumps(content, indent=2, ensure_ascii=False), encoding=encoding)
        else:
            path.write_text(str(content), encoding=encoding)

        logger.info(
            "Wrote pipeline output file",
            episode_id=self.episode_id,
            key=key,
            path=str(path),
        )

    def get_file_hash(self, key: str) -> str:
        try:
            target = self.get_input_path(key) if key in INPUT_KEYS else self.get_output_path(key)
            if not target.exists():
                return ""
            return hashlib.sha256(target.read_bytes()).hexdigest()[:16]
        except Exception as error:  # pragma: no cover - defensive
            logger.warning(
                "Failed to compute episode file hash",
                episode_id=self.episode_id,
                key=key,
                error=str(error),
            )
            return ""

    def file_exists(self, key: str) -> bool:
        try:
            target = self.get_input_path(key) if key in INPUT_KEYS else self.get_output_path(key)
            return target.exists()
        except ValueError:
            return False

    def list_files(self) -> Dict[str, Dict[str, Any]]:
        files: Dict[str, Dict[str, Any]] = {}
        for key, relative_path in self.file_config.files.items():
            full_path = self.base_path / relative_path
            if full_path.exists():
                stat = full_path.stat()
                files[key] = {
                    "exists": True,
                    "path": relative_path,
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "hash": self.get_file_hash(key),
                }
            else:
                files[key] = {"exists": False, "path": relative_path}
        return files

    def cleanup_outputs(self, stage: Optional[str] = None) -> None:
        stage_outputs: Dict[str, Iterable[str]] = {
            "summarization": ("summary", "keywords", "episode_context"),
            "scraping": ("tweets",),
            "classification": ("classified",),
            "response": ("responses",),
            "moderation": ("published",),
        }

        outputs_to_clean = stage_outputs.get(stage, DEFAULT_OUTPUT_KEYS)
        for key in outputs_to_clean:
            relative_path = self.file_config.files.get(key)
            if not relative_path:
                continue
            path = self.base_path / relative_path
            if path.exists():
                try:
                    path.unlink()
                    logger.info(
                        "Removed pipeline output",
                        episode_id=self.episode_id,
                        key=key,
                        path=str(path),
                    )
                except Exception as error:  # pragma: no cover - defensive
                    logger.warning(
                        "Failed to remove pipeline output",
                        episode_id=self.episode_id,
                        key=key,
                        error=str(error),
                    )


# Global repository instance shared across the backend.
episodes_repo = EpisodesRepository()


def get_episode_file_manager(
    episode_id: Optional[Union[int, str]] = None,
    *,
    episode_dir: Optional[str] = None,
) -> EpisodeFileManager:
    """Factory mirroring the previous helper while using the new service."""

    if episode_id is None:
        episode_id = "default"

    return EpisodeFileManager(episode_id, episode_dir=episode_dir)


__all__ = [
    "CLAUDE_FILE_MAP",
    "DEFAULT_OUTPUT_KEYS",
    "EpisodeFileManager",
    "EpisodesRepository",
    "episodes_repo",
    "get_episode_file_manager",
    "INPUT_KEYS",
]

